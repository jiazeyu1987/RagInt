#!/usr/bin/env python3
"""
Comprehensive test suite for TTS-ready text cleaning functionality
Tests both text_cleaner.py and tts_buffer.py modules
"""

import sys
import json
from pathlib import Path

# Add the ragflow_demo root to Python path
sys.path.append(str(Path(__file__).parent))

def test_text_cleaner():
    """Test TTSTextCleaner functionality"""
    print("=" * 60)
    print("Testing TTSTextCleaner Module")
    print("=" * 60)

    try:
        from text_cleaner import TTSTextCleaner, quick_clean_text, is_text_tts_ready

        # Test cases for various RAGFlow output formats
        test_cases = [
            # Basic markdown
            ("ËøôÊòØ‰∏Ä‰∏™**ÈáçË¶Å**ÁöÑÊ¶ÇÂøµ„ÄÇ", "basic_markdown"),

            # Mixed formatting
            ("Êô∫ËÉΩ‰ΩìÂÖ∑Â§á‰ª•‰∏ã**Ê†∏ÂøÉËÉΩÂäõ**Ôºö`Ëá™‰∏ªÊÑüÁü•`„ÄÅ`ÂÜ≥Á≠ñ`Âíå`Ë°åÂä®`„ÄÇ", "mixed_formatting"),

            # Code blocks
            ("‰ª•‰∏ãÊòØ‰∏Ä‰∏™‰æãÂ≠êÔºö\n```python\ndef agent():\n    return 'ÊÄùËÄÉ‰∏≠'\n```", "code_blocks"),

            # Lists and numbering
            ("1. **Ëá™‰∏ªÊÄß**ÔºöËÉΩÂ§üÁã¨Á´ãÊé®ÁêÜ\n2. **‰∫§‰∫íÊÄß**Ôºö‰∏éÁéØÂ¢É‰∫§‰∫í", "lists"),

            # Links
            ("ËØ∑ËÆøÈóÆ[RAGFlowÂÆòÁΩë](https://ragflow.io)‰∫ÜËß£Êõ¥Â§ö‰ø°ÊÅØ„ÄÇ", "links"),

            # Chinese punctuation
            ("Êô∫ËÉΩ‰ΩìÂÖ∑Â§á‰ª•‰∏ãÁâπÂæÅÔºöËá™‰∏ªÊÄß„ÄÅ‰∫§‰∫íÊÄß„ÄÅÂèçÂ∫îÊÄß„ÄÇ", "chinese_punctuation"),

            # Special characters and citations
            ("Ê†πÊçÆÁ†îÁ©∂[1]ÔºåÊô∫ËÉΩ‰ΩìÂ∏ÇÂú∫È¢ÑËÆ°‰ªé2024Âπ¥ÁöÑ51‰∫øÁæéÂÖÉÂ¢ûÈïøÂà∞2030Âπ¥„ÄÇ", "special_chars"),

            # Complex example (similar to RAGFlow output)
            ("""Êô∫ËÉΩ‰ΩìÊòØÊåáËÉΩÂ§ü**ÊÑüÁü•ÁéØÂ¢É**Âπ∂Âà©Áî®Â∑•ÂÖ∑ÈááÂèñË°åÂä®‰ª•ÂÆûÁé∞ÁâπÂÆöÁõÆÊ†áÁöÑ‰ª£ÁêÜ„ÄÇ

Êô∫ËÉΩ‰Ωì‰ª•Â§ßÊ®°Âûã‰∏∫Êô∫ËÉΩÂ∫ïÂ∫ßÔºåÂÖ∑Â§á‰ª•‰∏ãËÉΩÂäõÂíåÁâπÂæÅÔºö

1. **Ê†∏ÂøÉËÉΩÂäõ**Ôºö
   - **Ëá™‰∏ªÊÑüÁü•**ÔºöÈÄöËøá‰º†ÊÑüÂô®Á≠âËÆæÂ§áÊÑüÁü•Âë®Âõ¥ÁéØÂ¢É„ÄÇ
   - **ÁêÜËß£**ÔºöÁêÜËß£ÂíåÂ§ÑÁêÜÊù•Ëá™ÁéØÂ¢ÉÁöÑ‰ø°ÊÅØ„ÄÇ

2. **Âü∫Êú¨ÁâπÂæÅ**Ôºö
   - **Ëá™‰∏ªÊÄß**ÔºöËÉΩÂ§üÁã¨Á´ãÊé®ÁêÜ„ÄÅÂÜ≥Á≠ñÂíåÊâßË°å‰ªªÂä°„ÄÇ
   - **‰∫§‰∫íÊÄß**ÔºöËÉΩÂ§ü‰∏éÁéØÂ¢É„ÄÅÁî®Êà∑ÂíåÂÖ∂‰ªñÊô∫ËÉΩ‰ΩìËøõË°å‰∫§‰∫í„ÄÇ

3. **Â∫îÁî®Âú∫ÊôØ**Ôºö
   - **Â∑•‰∏öÂà∂ÈÄ†**ÔºöÁîü‰∫ß‰∏≠Êô∫ËÉΩÂàÜÊã£‰∏éÁâ©ÊñôÁÆ°ÁêÜ„ÄÇ
   - **ÂüéÂ∏ÇÁÆ°ÁêÜ**Ôºö‰∫§ÈÄöÁÆ°ÁêÜ„ÄÅÁ§æÂå∫Ê≤ªÁêÜÁ≠â„ÄÇ

Êõ¥Â§ö‰ø°ÊÅØËØ∑ÂèÇËÄÉ[Êô∫ËÉΩ‰ΩìÊñáÊ°£](https://example.com)ÂíåÊäÄÊúØËßÑËåÉ„ÄÇ""", "complex_example"),

            # Technical content with code
            ("‰ΩøÁî®`API_KEY`Âíå`dataset_ids`ÂèÇÊï∞Êù•ÂàùÂßãÂåñÔºö\n```python\nclient = RAGFlow(api_key='your_key')\n```", "technical_content"),
        ]

        print("\nTesting different cleaning levels:")
        print("-" * 40)

        for test_text, test_name in test_cases:
            print(f"\nTest: {test_name}")
            print(f"Original: {test_text[:100]}{'...' if len(test_text) > 100 else ''}")

            # Test different cleaning levels
            for level in ['basic', 'standard', 'aggressive']:
                try:
                    cleaner = TTSTextCleaner(cleaning_level=level)
                    cleaned = cleaner.clean_streaming_chunk(test_text, is_partial=False)
                    print(f"{level.capitalize()}: {cleaned[:80]}{'...' if len(cleaned) > 80 else ''}")

                    # Test if result is TTS-ready
                    if is_text_tts_ready(cleaned):
                        print(f"  [OK] TTS-ready: Yes")
                    else:
                        print(f"  [WARN] TTS-ready: No")
                except Exception as e:
                    print(f"  [ERROR] in {level}: {str(e)}")

            print("-" * 30)

        # Test streaming functionality
        print(f"\nTesting streaming functionality:")
        print("-" * 40)

        streaming_text = "ËøôÊòØ‰∏Ä‰∏™**ÊµãËØï**ÔºåÁî®‰∫éÈ™åËØÅ`ÊµÅÂºè`ÊñáÊú¨Â§ÑÁêÜÂäüËÉΩÊòØÂê¶Ê≠£Â∏∏Â∑•‰Ωú„ÄÇ"
        cleaner = TTSTextCleaner()

        # Simulate streaming chunks
        chunks = ["ËøôÊòØ‰∏Ä‰∏™**", "ÊµãËØï**ÔºåÁî®‰∫éÈ™åËØÅ`ÊµÅÂºè", "ÊñáÊú¨Â§ÑÁêÜÂäüËÉΩÊòØÂê¶Ê≠£Â∏∏Â∑•‰Ωú„ÄÇ"]

        print(f"Original text: {streaming_text}")
        print("Streaming processing:")

        accumulated_cleaned = ""
        for i, chunk in enumerate(chunks, 1):
            cleaned_chunk = cleaner.clean_streaming_chunk(chunk, is_partial=True)
            accumulated_cleaned += cleaned_chunk
            print(f"  Chunk {i}: '{chunk}' -> '{cleaned_chunk}'")

        print(f"Final accumulated: '{accumulated_cleaned}'")

        # Test quick clean function
        print(f"\nTesting quick clean function:")
        print("-" * 40)

        quick_text = "**Quick test** for `convenience` function."
        quick_cleaned = quick_clean_text(quick_text, 'standard')
        print(f"Original: '{quick_text}'")
        print(f"Quick cleaned: '{quick_cleaned}'")

        print("\n[SUCCESS] TTSTextCleaner tests completed successfully!")
        return True

    except ImportError as e:
        print(f"[ERROR] Import Error: {str(e)}")
        return False
    except Exception as e:
        print(f"[ERROR] Test Error: {str(e)}")
        return False


def test_tts_buffer():
    """Test TTSBuffer functionality"""
    print("\n" + "=" * 60)
    print("Testing TTSBuffer Module")
    print("=" * 60)

    try:
        from tts_buffer import TTSBuffer, SemanticChunker

        # Test cases for buffer chunking
        test_sequences = [
            # Simple sentences
            (["ËøôÊòØÁ¨¨‰∏ÄÂè•ËØù„ÄÇ", "ËøôÊòØÁ¨¨‰∫åÂè•ËØùÔºÅ", "ËøôÊòØÁ¨¨‰∏âÂè•ËØùÔºü"], "simple_sentences"),

            # Mixed content
            (["Êô∫ËÉΩ‰ΩìÂÖ∑Â§á", "‰ª•‰∏ãÁâπÂæÅÔºö", "Ëá™‰∏ªÊÄß„ÄÅ", "‰∫§‰∫íÊÄßÂíå", "ÈÄÇÂ∫îÊÄß„ÄÇ"], "mixed_content"),

            # Longer content requiring size-based chunking
            (["ËøôÊòØ‰∏Ä‰∏™ÂæàÈïøÁöÑÂè•Â≠êÔºå", "ÂåÖÂê´‰∫ÜÂ§ö‰∏™ÂàÜÂè•Âíå", "‰∏çÂêåÁöÑÂÜÖÂÆπÈÉ®ÂàÜÔºå", "ÈúÄË¶ÅÊô∫ËÉΩÂú∞ËøõË°åÂàÜÂâ≤Â§ÑÁêÜ„ÄÇ"], "long_content"),

            # Complex conversation-style content
            (["Ê†πÊçÆÁ†îÁ©∂ÔºåÊô∫ËÉΩ‰ΩìÂ∏ÇÂú∫È¢ÑËÆ°", "‰ªé2024Âπ¥ÁöÑ51‰∫øÁæéÂÖÉ", "Â¢ûÈïøÂà∞2030Âπ¥ÁöÑ471‰∫øÁæéÂÖÉÔºå", "Âπ¥Â§çÂêàÂ¢ûÈïøÁéáËææ44.8%„ÄÇ"], "conversation_style"),
        ]

        print("\nTesting TTSBuffer with different content sequences:")
        print("-" * 50)

        for text_chunks, test_name in test_sequences:
            print(f"\nTest: {test_name}")
            print(f"Input chunks: {text_chunks}")

            buffer = TTSBuffer(max_chunk_size=50, language='zh-CN')
            all_ready_chunks = []

            for i, chunk in enumerate(text_chunks, 1):
                ready_chunks = buffer.add_cleaned_chunk(chunk)
                print(f"  Added '{chunk}' -> Ready chunks: {ready_chunks}")
                all_ready_chunks.extend(ready_chunks)

            # Finalize and get remaining chunks
            final_chunks = buffer.finalize()
            all_ready_chunks.extend(final_chunks)

            print(f"  Final chunks: {final_chunks}")
            print(f"  All TTS-ready chunks: {all_ready_chunks}")
            print(f"  Buffer status: {buffer.get_buffer_status()}")

            buffer.reset()
            print("-" * 30)

        # Test SemanticChunker
        print(f"\nTesting SemanticChunker:")
        print("-" * 30)

        chunker = SemanticChunker(language='zh-CN')
        semantic_chunks = []

        test_text = "ËøôÊòØÁ¨¨‰∏ÄÂè•ËØù„ÄÇËøôÊòØÁ¨¨‰∫åÂè•ËØùÔºÅËøôÊòØÁ¨¨‰∏âÂè•ËØùÔºü"
        for char in test_text:
            # Simulate character-by-character streaming
            result = chunker.add_text(char)
            if result:
                semantic_chunks.extend(result)

        print(f"Input text: '{test_text}'")
        print(f"Semantic chunks: {semantic_chunks}")

        print("\n‚úÖ TTSBuffer tests completed successfully!")
        return True

    except ImportError as e:
        print(f"‚ùå Import Error: {str(e)}")
        return False
    except Exception as e:
        print(f"‚ùå Test Error: {str(e)}")
        return False


def test_integration():
    """Test integration between text_cleaner and tts_buffer"""
    print("\n" + "=" * 60)
    print("Testing Integration: TextCleaner + TTSBuffer")
    print("=" * 60)

    try:
        from text_cleaner import TTSTextCleaner
        from tts_buffer import TTSBuffer

        # Simulate RAGFlow streaming output
        ragflow_chunks = [
            "Êô∫ËÉΩ‰ΩìÊòØÊåáËÉΩÂ§ü**ÊÑüÁü•ÁéØÂ¢É**",
            "Âπ∂Âà©Áî®Â∑•ÂÖ∑ÈááÂèñË°åÂä®‰ª•ÂÆûÁé∞",
            "ÁâπÂÆöÁõÆÊ†áÁöÑ‰ª£ÁêÜ„ÄÇÂÆÉ‰ª¨ÂÖ∑Â§á‰ª•‰∏ã",
            "ÁâπÂæÅÔºö1. **Ëá™‰∏ªÊÄß**ÔºöËÉΩÂ§üÁã¨Á´ã",
            "Êé®ÁêÜ„ÄÅÂÜ≥Á≠ñÂíåÊâßË°å‰ªªÂä°„ÄÇ",
            "2. **‰∫§‰∫íÊÄß**ÔºöËÉΩÂ§ü‰∏éÁéØÂ¢É„ÄÅ",
            "Áî®Êà∑ÂíåÂÖ∂‰ªñÊô∫ËÉΩ‰ΩìËøõË°å‰∫§‰∫í„ÄÇ"
        ]

        print(f"Simulating RAGFlow streaming output:")
        print(f"Chunks: {ragflow_chunks}")

        # Initialize components
        cleaner = TTSTextCleaner(language='zh-CN', cleaning_level='standard')
        buffer = TTSBuffer(max_chunk_size=100, language='zh-CN')

        all_tts_chunks = []

        print(f"\nProcessing with integration:")
        print("-" * 40)

        for i, chunk in enumerate(ragflow_chunks, 1):
            print(f"\nStep {i}: Processing chunk '{chunk}'")

            # Clean the chunk
            cleaned_chunk = cleaner.clean_streaming_chunk(chunk, is_partial=True)
            print(f"  Cleaned: '{cleaned_chunk}'")

            # Add to buffer
            tts_ready_chunks = buffer.add_cleaned_chunk(cleaned_chunk)
            if tts_ready_chunks:
                print(f"  TTS-ready chunks: {tts_ready_chunks}")
                all_tts_chunks.extend(tts_ready_chunks)
            else:
                print(f"  No TTS-ready chunks yet")

        # Finalize buffer
        final_chunks = buffer.finalize()
        all_tts_chunks.extend(final_chunks)

        print(f"\nFinal TTS-ready chunks: {all_tts_chunks}")
        print(f"Complete clean text: '{buffer.get_complete_clean_text()}'")

        # Verify TTS readiness
        from text_cleaner import is_text_tts_ready
        for i, chunk in enumerate(all_tts_chunks, 1):
            is_ready = is_text_tts_ready(chunk)
            print(f"Chunk {i}: '{chunk}' -> TTS-ready: {is_ready}")

        print("\n‚úÖ Integration tests completed successfully!")
        return True

    except Exception as e:
        print(f"‚ùå Integration Test Error: {str(e)}")
        return False


def test_config_integration():
    """Test configuration file integration"""
    print("\n" + "=" * 60)
    print("Testing Configuration Integration")
    print("=" * 60)

    try:
        # Load configuration
        config_path = Path(__file__).parent / "ragflow_config.json"

        if not config_path.exists():
            print(f"‚ùå Configuration file not found: {config_path}")
            return False

        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

        print(f"‚úÖ Configuration loaded successfully")
        print(f"Text cleaning config: {config.get('text_cleaning', {})}")

        # Test configuration validation
        required_fields = ['enabled', 'language', 'cleaning_level']
        text_cleaning_config = config.get('text_cleaning', {})

        missing_fields = [field for field in required_fields if field not in text_cleaning_config]
        if missing_fields:
            print(f"‚ö†Ô∏è  Missing config fields: {missing_fields}")
        else:
            print(f"‚úÖ All required fields present")

        # Test component initialization with config
        from text_cleaner import TTSTextCleaner
        from tts_buffer import TTSBuffer

        # Initialize with config values
        cleaner = TTSTextCleaner(
            language=text_cleaning_config.get('language', 'zh-CN'),
            cleaning_level=text_cleaning_config.get('cleaning_level', 'standard')
        )

        buffer = TTSBuffer(
            max_chunk_size=text_cleaning_config.get('max_chunk_size', 200),
            language=text_cleaning_config.get('language', 'zh-CN')
        )

        print(f"‚úÖ Components initialized with config successfully")

        # Test with sample text
        sample_text = "ËøôÊòØ‰∏Ä‰∏™**ÊµãËØï**ÈÖçÁΩÆÈõÜÊàêÁöÑÂè•Â≠ê„ÄÇ"
        cleaned = cleaner.clean_streaming_chunk(sample_text, is_partial=False)
        buffer.add_cleaned_chunk(cleaned)

        print(f"Sample text: '{sample_text}'")
        print(f"Cleaned: '{cleaned}'")
        print(f"Buffer status: {buffer.get_buffer_status()}")

        print("\n‚úÖ Configuration integration tests completed successfully!")
        return True

    except Exception as e:
        print(f"‚ùå Configuration Test Error: {str(e)}")
        return False


def main():
    """Run all tests"""
    print("RAGFlow TTS Text Cleaning Test Suite")
    print("=" * 60)

    tests = [
        ("Text Cleaner", test_text_cleaner),
        ("TTS Buffer", test_tts_buffer),
        ("Integration", test_integration),
        ("Configuration", test_config_integration)
    ]

    results = []

    for test_name, test_func in tests:
        try:
            print(f"\n{'='*20} {test_name} {'='*20}")
            result = test_func()
            results.append((test_name, result))
        except Exception as e:
            print(f"‚ùå {test_name} test failed with exception: {str(e)}")
            results.append((test_name, False))

    # Summary
    print("\n" + "=" * 60)
    print("TEST SUMMARY")
    print("=" * 60)

    passed = 0
    total = len(results)

    for test_name, result in results:
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{test_name}: {status}")
        if result:
            passed += 1

    print(f"\nResults: {passed}/{total} tests passed")

    if passed == total:
        print("üéâ All tests passed! Text cleaning system is ready for TTS integration.")
    else:
        print("‚ö†Ô∏è  Some tests failed. Please review the implementation.")

    return passed == total


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)