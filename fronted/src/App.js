import React, { useState, useRef } from 'react';
import './App.css';

async function playWavStreamViaWebAudio(url, audioContextRef, currentAudioRef, fallbackPlay) {
  const AudioContextClass = window.AudioContext || window.webkitAudioContext;
  if (!AudioContextClass) {
    if (fallbackPlay) return fallbackPlay();
    throw new Error('WebAudio is not supported');
  }

  if (!audioContextRef.current) {
    audioContextRef.current = new AudioContextClass();
  }
  const audioCtx = audioContextRef.current;
  if (audioCtx.state === 'suspended') {
    try {
      await audioCtx.resume();
    } catch (_) {
      // ignore
    }
  }

  const abortController = new AbortController();
  currentAudioRef.current = { stop: () => abortController.abort() };

  const sources = [];
  const stopAllSources = () => {
    while (sources.length) {
      const src = sources.pop();
      try {
        src.stop(0);
      } catch (_) {
        // ignore
      }
    }
  };

  const parseWavHeader = (headerBytes) => {
    const view = new DataView(headerBytes.buffer, headerBytes.byteOffset, headerBytes.byteLength);
    const readFourCC = (offset) =>
      String.fromCharCode(view.getUint8(offset), view.getUint8(offset + 1), view.getUint8(offset + 2), view.getUint8(offset + 3));

    if (headerBytes.byteLength < 44) throw new Error('WAV header too short');
    if (readFourCC(0) !== 'RIFF' || readFourCC(8) !== 'WAVE') throw new Error('Not a RIFF/WAVE stream');

    let channels = 1;
    let sampleRate = 32000;
    let bitsPerSample = 16;
    let dataOffset = 44;

    let offset = 12;
    while (offset + 8 <= headerBytes.byteLength) {
      const chunkId = readFourCC(offset);
      const chunkSize = view.getUint32(offset + 4, true);
      const payloadOffset = offset + 8;

      if (chunkId === 'fmt ' && payloadOffset + 16 <= headerBytes.byteLength) {
        channels = view.getUint16(payloadOffset + 2, true);
        sampleRate = view.getUint32(payloadOffset + 4, true);
        bitsPerSample = view.getUint16(payloadOffset + 14, true);
      } else if (chunkId === 'data') {
        dataOffset = payloadOffset;
        break;
      }

      offset = payloadOffset + chunkSize;
      if (offset % 2 === 1) offset += 1;
    }

    return { channels, sampleRate, bitsPerSample, dataOffset };
  };

  try {
    const response = await fetch(url, { signal: abortController.signal });
    if (!response.ok || !response.body) throw new Error(`TTS stream HTTP error: ${response.status}`);

    const reader = response.body.getReader();
    let headerBuffer = new Uint8Array(0);
    let wavInfo = null;
    let pcmRemainder = new Uint8Array(0);
    let nextStartTime = audioCtx.currentTime + 0.05;

    const schedulePcmChunk = (pcmBytes) => {
      if (!wavInfo) return;
      if (wavInfo.bitsPerSample !== 16) throw new Error(`Unsupported bitsPerSample: ${wavInfo.bitsPerSample}`);

      const blockAlign = wavInfo.channels * 2;
      const usableBytes = pcmBytes.byteLength - (pcmBytes.byteLength % blockAlign);
      if (usableBytes <= 0) return;

      const frames = usableBytes / blockAlign;
      const audioBuffer = audioCtx.createBuffer(wavInfo.channels, frames, wavInfo.sampleRate);

      const aligned = pcmBytes.slice(0, usableBytes);
      const int16 = new Int16Array(aligned.buffer, aligned.byteOffset, aligned.byteLength / 2);

      for (let ch = 0; ch < wavInfo.channels; ch++) {
        const channelData = audioBuffer.getChannelData(ch);
        for (let i = 0; i < frames; i++) {
          channelData[i] = int16[i * wavInfo.channels + ch] / 32768;
        }
      }

      const src = audioCtx.createBufferSource();
      src.buffer = audioBuffer;
      src.connect(audioCtx.destination);
      sources.push(src);

      const when = Math.max(nextStartTime, audioCtx.currentTime + 0.01);
      src.start(when);
      nextStartTime = when + audioBuffer.duration;
    };

    while (true) {
      const { done, value } = await reader.read();
      if (done) break;
      if (!value || value.byteLength === 0) continue;

      let chunk = value;

      if (!wavInfo) {
        const merged = new Uint8Array(headerBuffer.byteLength + chunk.byteLength);
        merged.set(headerBuffer, 0);
        merged.set(chunk, headerBuffer.byteLength);
        headerBuffer = merged;

        if (headerBuffer.byteLength < 44) continue;

        const headerForParse = headerBuffer.byteLength > 4096 ? headerBuffer.slice(0, 4096) : headerBuffer;
        wavInfo = parseWavHeader(headerForParse);

        const dataStart = wavInfo.dataOffset;
        if (headerBuffer.byteLength > dataStart) {
          schedulePcmChunk(headerBuffer.slice(dataStart));
        }

        headerBuffer = new Uint8Array(0);
        continue;
      }

      if (pcmRemainder.byteLength) {
        const merged = new Uint8Array(pcmRemainder.byteLength + chunk.byteLength);
        merged.set(pcmRemainder, 0);
        merged.set(chunk, pcmRemainder.byteLength);
        pcmRemainder = new Uint8Array(0);
        chunk = merged;
      }

      const blockAlign = wavInfo.channels * 2;
      const usableBytes = chunk.byteLength - (chunk.byteLength % blockAlign);
      if (usableBytes > 0) {
        schedulePcmChunk(chunk.slice(0, usableBytes));
      }
      const leftover = chunk.byteLength - usableBytes;
      if (leftover > 0) {
        pcmRemainder = chunk.slice(usableBytes);
      }
    }

    const remainingMs = Math.max(0, (nextStartTime - audioCtx.currentTime) * 1000);
    await new Promise((resolve) => setTimeout(resolve, remainingMs + 50));
  } catch (err) {
    stopAllSources();
    if (abortController.signal.aborted) return;
    if (fallbackPlay) return fallbackPlay();
    throw err;
  } finally {
    stopAllSources();
  }
}

function App() {
  const [isRecording, setIsRecording] = useState(false);
  const [question, setQuestion] = useState('');
  const [answer, setAnswer] = useState('');
  const [isLoading, setIsLoading] = useState(false);
  const [queueStatus, setQueueStatus] = useState('');
  const mediaRecorderRef = useRef(null);
  const audioChunksRef = useRef([]);

  // åŸå§‹æ–‡æœ¬é˜Ÿåˆ—å’Œé¢„ç”ŸæˆéŸ³é¢‘é˜Ÿåˆ—
  const ttsTextQueueRef = useRef([]);
  const ttsAudioQueueRef = useRef([]);

  // å·¥ä½œçº¿ç¨‹å¼•ç”¨
  const ttsGeneratorPromiseRef = useRef(null);
  const ttsPlayerPromiseRef = useRef(null);

  const ragflowDoneRef = useRef(false);
  const runIdRef = useRef(0);
  const currentAudioRef = useRef(null);
  const receivedSegmentsRef = useRef(false);
  const audioContextRef = useRef(null);

  // TTSé¢„ç”Ÿæˆé…ç½®
  const MAX_PRE_GENERATE_COUNT = 2; // æœ€å¤šé¢„ç”Ÿæˆ2æ®µéŸ³é¢‘

  // æ›´æ–°é˜Ÿåˆ—çŠ¶æ€æ˜¾ç¤º
  const updateQueueStatus = () => {
    const textCount = ttsTextQueueRef.current.length;
    const audioCount = ttsAudioQueueRef.current.length;
    const generatorRunning = !!ttsGeneratorPromiseRef.current;
    const playerRunning = !!ttsPlayerPromiseRef.current;

    setQueueStatus(
      `ğŸ“å¾…ç”Ÿæˆ: ${textCount} | ğŸ”Šé¢„ç”Ÿæˆ: ${audioCount} | ` +
      `${generatorRunning ? 'ğŸµç”Ÿæˆä¸­' : 'â¸ï¸ç”Ÿæˆç©ºé—²'} | ` +
      `${playerRunning ? 'ğŸ”Šæ’­æ”¾ä¸­' : 'â¸ï¸æ’­æ”¾ç©ºé—²'}`
    );
  };

  // å¯åŠ¨é˜Ÿåˆ—çŠ¶æ€ç›‘æ§
  const startStatusMonitor = (runId) => {
    const interval = setInterval(() => {
      if (runIdRef.current === runId && (isLoading || ttsGeneratorPromiseRef.current || ttsPlayerPromiseRef.current)) {
        updateQueueStatus();
      } else {
        setQueueStatus('');
        clearInterval(interval);
      }
    }, 200); // æ¯200msæ›´æ–°ä¸€æ¬¡çŠ¶æ€
  };

  const startRecording = async () => {
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const mediaRecorder = new MediaRecorder(stream);
      mediaRecorderRef.current = mediaRecorder;
      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        audioChunksRef.current.push(event.data);
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, { type: 'audio/wav' });
        stream.getTracks().forEach(track => track.stop());
        await processAudio(audioBlob);
      };

      mediaRecorder.start();
      setIsRecording(true);
    } catch (err) {
      console.error('Error accessing microphone:', err);
    }
  };

  const stopRecording = () => {
    if (mediaRecorderRef.current && isRecording) {
      mediaRecorderRef.current.stop();
      setIsRecording(false);
    }
  };

  const processAudio = async (audioBlob) => {
    setIsLoading(true);
    try {
      const formData = new FormData();
      formData.append('audio', audioBlob);

      const response = await fetch('http://localhost:8000/api/speech_to_text', {
        method: 'POST',
        body: formData
      });

      const result = await response.json();
      const text = result.text || '';

      if (text) {
        await askQuestion(text);
      } else {
        setIsLoading(false);
      }
    } catch (err) {
      console.error('Error processing audio:', err);
      setIsLoading(false);
    }
  };

  const askQuestion = async (text) => {
    const runId = ++runIdRef.current;
    const requestId = `ask_${runId}_${Date.now()}`;
    let segmentIndex = 0;
    setQuestion(text);
    setAnswer('');
    setIsLoading(true);

    // æ¸…ç©ºæ‰€æœ‰é˜Ÿåˆ—
    ttsTextQueueRef.current = [];
    ttsAudioQueueRef.current = [];
    ragflowDoneRef.current = false;
    receivedSegmentsRef.current = false;

    // å¯åŠ¨çŠ¶æ€ç›‘æ§
    startStatusMonitor(runId);

    // åœæ­¢å½“å‰æ’­æ”¾çš„éŸ³é¢‘
    if (currentAudioRef.current) {
      try {
        if (typeof currentAudioRef.current.stop === 'function') {
          currentAudioRef.current.stop();
        } else if (typeof currentAudioRef.current.pause === 'function') {
          currentAudioRef.current.pause();
          currentAudioRef.current.src = '';
        }
      } catch (_) {
        // ignore
      }
      currentAudioRef.current = null;
    }

    // ç»ˆæ­¢ä¹‹å‰çš„å·¥ä½œçº¿ç¨‹
    if (ttsGeneratorPromiseRef.current) {
      ttsGeneratorPromiseRef.current = null;
    }
    if (ttsPlayerPromiseRef.current) {
      ttsPlayerPromiseRef.current = null;
    }

    const sleep = (ms) => new Promise((resolve) => setTimeout(resolve, ms));

    // TTSéŸ³é¢‘ç”Ÿæˆå‡½æ•°
    const generateAudioSegmentUrl = (segmentText) => {
      try {
        console.log(`ğŸµ å¼€å§‹ç”ŸæˆéŸ³é¢‘: "${segmentText.substring(0, 30)}..."`);
        const seg = String(segmentText || '').trim();
        if (!seg) return null;
        const url = new URL('http://localhost:8000/api/text_to_speech_stream');
        url.searchParams.set('text', seg);
        url.searchParams.set('request_id', requestId);
        url.searchParams.set('segment_index', String(segmentIndex++));
        return url.toString();


        console.log(`âœ… éŸ³é¢‘ç”Ÿæˆå®Œæˆ: ${audioBlob.size} bytes`);
      } catch (err) {
        console.error(`âŒ éŸ³é¢‘ç”Ÿæˆå¤±è´¥: "${segmentText}"`, err);
        return null;
      }
    };

    // TTSéŸ³é¢‘æ’­æ”¾å‡½æ•°
    const playAudioUrl = async (audioUrl, segmentText) => {
      if (!audioUrl) return;
      try {
        console.log(`ğŸ”Š å¼€å§‹æ’­æ”¾: "${segmentText.substring(0, 30)}..."`);
        await new Promise((resolve, reject) => {
          const audio = new Audio(audioUrl);
          currentAudioRef.current = audio;

          audio.onended = () => {
            console.log(`âœ… æ’­æ”¾å®Œæˆ: "${segmentText.substring(0, 30)}..."`);
            resolve();
          };
          audio.onerror = () => reject(new Error('Audio playback failed'));

          audio.play().catch(reject);
        });
      } catch (err) {
        console.error(`âŒ æ’­æ”¾å¤±è´¥: "${segmentText}"`, err);
      } finally {
        if (currentAudioRef.current) {
          currentAudioRef.current = null;
        }
      }
    };

    // TTSéŸ³é¢‘ç”Ÿæˆå·¥ä½œçº¿ç¨‹ - åå°é¢„ç”ŸæˆéŸ³é¢‘
    const startTTSGenerator = () => {
      if (ttsGeneratorPromiseRef.current) return;

      ttsGeneratorPromiseRef.current = (async () => {
        while (runIdRef.current === runId) {
          // å¦‚æœéŸ³é¢‘é˜Ÿåˆ—å·²ç»æœ‰è¶³å¤Ÿçš„é¢„ç”ŸæˆéŸ³é¢‘ï¼Œç­‰å¾…
          if (ttsAudioQueueRef.current.length >= MAX_PRE_GENERATE_COUNT) {
            await sleep(50);
            continue;
          }

          // æ£€æŸ¥æ˜¯å¦æœ‰å¾…ç”Ÿæˆçš„æ–‡æœ¬
          const nextSegment = ttsTextQueueRef.current[0]; // æŸ¥çœ‹ä½†ä¸ç§»é™¤
          if (!nextSegment) {
            if (ragflowDoneRef.current) {
              console.log('ğŸ TTSç”Ÿæˆå™¨: æ‰€æœ‰æ–‡æœ¬å·²å¤„ç†å®Œæ¯•');
              break;
            }
            await sleep(50);
            continue;
          }

          // ç§»é™¤æ–‡æœ¬å¹¶ç”ŸæˆéŸ³é¢‘
          ttsTextQueueRef.current.shift();
          const audioUrl = generateAudioSegmentUrl(nextSegment);

          if (audioUrl) {
            ttsAudioQueueRef.current.push({
              text: nextSegment,
              url: audioUrl
            });
          }

          // æ£€æŸ¥æ˜¯å¦åº”è¯¥å¯åŠ¨æ’­æ”¾å™¨
          if (!ttsPlayerPromiseRef.current && ttsAudioQueueRef.current.length > 0) {
            startTTSPlayer();
          }
        }
      })()
        .catch((err) => {
          console.error('âŒ TTSç”Ÿæˆçº¿ç¨‹å‡ºé”™:', err);
        })
        .finally(() => {
          ttsGeneratorPromiseRef.current = null;
        });
    };

    // TTSéŸ³é¢‘æ’­æ”¾å·¥ä½œçº¿ç¨‹ - ä¸“é—¨è´Ÿè´£æ’­æ”¾
    const startTTSPlayer = () => {
      if (ttsPlayerPromiseRef.current) return;

      ttsPlayerPromiseRef.current = (async () => {
        while (runIdRef.current === runId) {
          const audioItem = ttsAudioQueueRef.current.shift();
          if (!audioItem) {
            // æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å·¥ä½œéƒ½å·²å®Œæˆ
            if (ragflowDoneRef.current && !ttsGeneratorPromiseRef.current) {
              console.log('ğŸ TTSæ’­æ”¾å™¨: æ‰€æœ‰éŸ³é¢‘æ’­æ”¾å®Œæ¯•');
              break;
            }
            await sleep(50);
            continue;
          }

          await playWavStreamViaWebAudio(
            audioItem.url,
            audioContextRef,
            currentAudioRef,
            () => playAudioUrl(audioItem.url, audioItem.text)
          );
        }
      })()
        .catch((err) => {
          console.error('âŒ TTSæ’­æ”¾çº¿ç¨‹å‡ºé”™:', err);
        })
        .finally(() => {
          if (runIdRef.current === runId) {
            setIsLoading(false);
          }
          ttsPlayerPromiseRef.current = null;
        });
    };

    try {
      const response = await fetch('http://localhost:8000/api/ask', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({ question: text, request_id: requestId })
      });

      const reader = response.body.getReader();
      const decoder = new TextDecoder();
      let fullAnswer = '';
      let sseBuffer = '';

      while (true) {
        const { done, value } = await reader.read();
        if (done) break;

        sseBuffer += decoder.decode(value, { stream: true });
        const lines = sseBuffer.split('\n');
        sseBuffer = lines.pop() || '';

        for (const line of lines) {
          const trimmed = line.trim();
          if (trimmed.startsWith('data: ')) {
            try {
              const data = JSON.parse(trimmed.slice(6));
              if (data.chunk && !data.done) {
                fullAnswer += data.chunk;
                setAnswer(fullAnswer);
              }

              if (data.segment && !data.done) {
                const seg = String(data.segment).trim();
                if (seg) {
                  receivedSegmentsRef.current = true;
                  ttsTextQueueRef.current.push(seg);
                  console.log(`ğŸ“ æ”¶åˆ°æ–‡æœ¬æ®µè½: "${seg.substring(0, 30)}..."`);
                  startTTSGenerator();
                }
              }

              if (data.done) {
                if (!receivedSegmentsRef.current && fullAnswer.trim()) {
                  ttsTextQueueRef.current.push(fullAnswer.trim());
                  console.log(`ğŸ“ æ”¶åˆ°å®Œæ•´æ–‡æœ¬: "${fullAnswer.substring(0, 30)}..."`);
                }
                ragflowDoneRef.current = true;
                console.log('ğŸ“š RAGFlowå“åº”å®Œæˆï¼Œç­‰å¾…TTSå¤„ç†å®Œæ¯•');
                startTTSGenerator();

                // ç­‰å¾…TTSç”Ÿæˆå™¨å®Œæˆ
                if (ttsGeneratorPromiseRef.current) {
                  await ttsGeneratorPromiseRef.current;
                }

                // ç­‰å¾…TTSæ’­æ”¾å™¨å®Œæˆ
                if (ttsPlayerPromiseRef.current) {
                  await ttsPlayerPromiseRef.current;
                }
                return;
              }
            } catch (err) {
              console.error('Error parsing chunk:', err);
            }
          }
        }
      }
    } catch (err) {
      console.error('Error asking question:', err);
      setIsLoading(false);
    }
  };

  const handleTextSubmit = async (e) => {
    e.preventDefault();
    if (question.trim() && !isLoading) {
      await askQuestion(question);
    }
  };

  return (
    <div className="app">
      <div className="container">
        <h1>AIè¯­éŸ³é—®ç­”</h1>

        <div className="input-section">
          <div className="voice-input">
            <button
              className={`record-btn ${isRecording ? 'recording' : ''}`}
              onMouseDown={startRecording}
              onMouseUp={stopRecording}
              onTouchStart={startRecording}
              onTouchEnd={stopRecording}
              disabled={isLoading}
            >
              {isRecording ? 'ğŸ”´ å½•éŸ³ä¸­...' : 'ğŸ¤ æŒ‰ä½è¯´è¯'}
            </button>
          </div>

          <form className="text-input" onSubmit={handleTextSubmit}>
            <input
              type="text"
              value={question}
              onChange={(e) => setQuestion(e.target.value)}
              placeholder="æˆ–è€…è¾“å…¥æ–‡å­—é—®é¢˜..."
              disabled={isLoading}
            />
            <button type="submit" disabled={isLoading}>
              å‘é€
            </button>
          </form>
        </div>

        {question && (
          <div className="question-section">
            <h3>é—®é¢˜: {question}</h3>
          </div>
        )}

        {answer && (
          <div className="answer-section">
            <h3>å›ç­”:</h3>
            <p>{answer}</p>
          </div>
        )}

        {isLoading && (
          <div className="loading">
            å¤„ç†ä¸­...
          </div>
        )}

        {queueStatus && (
          <div className="queue-status">
            <small>{queueStatus}</small>
          </div>
        )}
      </div>
    </div>
  );
}

export default App;
